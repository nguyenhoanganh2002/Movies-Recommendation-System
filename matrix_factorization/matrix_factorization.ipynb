{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "from scipy.sparse import csc_matrix\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.parameter import Parameter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "torch.manual_seed(1284)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(path='./', delimiter=',', max_user = None):\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    print(df.columns)\n",
    "    rows_with_nan = df[df.isna().any(axis=1)]\n",
    "    print(rows_with_nan)\n",
    "    df = df[['userId', 'movieId', 'rating']]\n",
    "\n",
    "    # print( df['movieId'])\n",
    "    if max_user:\n",
    "        df = df.loc[df['userId'] <= max_user]\n",
    "    def discrete(column):\n",
    "        sorted_values = sorted(df[column].unique(), reverse=True)\n",
    "        def custom_rank(value):\n",
    "            return sorted_values.index(value) + 1\n",
    "        return custom_rank\n",
    "\n",
    "    # df['userId'] = df['userId'].apply(discrete('userId'))\n",
    "    # df['movieId'] = df['movieId'].apply(discrete('movieId'))\n",
    "    # df.to_csv('discreted_link_ratings.csv')\n",
    "    n_u = df['userId'].nunique()\n",
    "    n_m = df['movieId'].nunique()\n",
    "    # Chia tập train từ tập dữ liệu\n",
    "    train, df_temp = train_test_split(df, test_size=0.4, shuffle=True, random_state=42)\n",
    "\n",
    "    # Chia tập test và tập validation từ tập dữ liệu còn lại\n",
    "    test, val = train_test_split(df_temp, test_size=0.5, shuffle=True, random_state=42)\n",
    "    print(train.shape, val.shape, test.shape, n_m, n_u)\n",
    "    # print(train.iloc[0])\n",
    "    # print(df.iloc[0])\n",
    "    train_r = np.zeros((n_m, n_u))\n",
    "    val_r = np.zeros((n_m, n_u))\n",
    "    test_r = np.zeros((n_m, n_u))\n",
    "\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_val = val.shape[0]  # num of valid ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    for i in range(n_train):\n",
    "        # print(train.iloc[i,1]-1, train.iloc[i,0]-1)\n",
    "        train_r[train.iloc[i,1]-1, train.iloc[i,0]-1] = train.iloc[i,2]\n",
    "\n",
    "    for i in range(n_val):\n",
    "        # print(train.iloc[i,1]-1, train.iloc[i,0]-1)\n",
    "        val_r[val.iloc[i,1]-1, val.iloc[i,0]-1] = val.iloc[i,2]\n",
    "\n",
    "    for i in range(n_test):\n",
    "        test_r[test.iloc[i,1]-1, test.iloc[i,0]-1] = test.iloc[i,2]\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "    val_m = np.greater(val_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of valid ratings: {}'.format(n_val))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "    return n_m, n_u, train_r, train_m, val_r, val_m, test_r, test_m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = 'discreted_link_ratings_p0.csv'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['userId', 'movieId', 'rating'], dtype='object')\n",
      "Empty DataFrame\n",
      "Columns: [userId, movieId, rating]\n",
      "Index: []\n",
      "(117680, 3) (39227, 3) (39227, 3) 14363 1999\n",
      "data matrix loaded\n",
      "num of users: 1999\n",
      "num of movies: 14363\n",
      "num of training ratings: 117680\n",
      "num of valid ratings: 39227\n",
      "num of test ratings: 39227\n"
     ]
    }
   ],
   "source": [
    "# Data Load\n",
    "path = data_path\n",
    "n_m, n_u, train_r, train_m, val_r, val_m, test_r, test_m = load_data_from_csv(path=path, delimiter='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common hyperparameter settings\n",
    "n_hid = 500 # size of hidden layers\n",
    "n_dim = 5 # inner AE embedding size\n",
    "n_layers = 3 # number of hidden layers\n",
    "gk_size = 3 # width=height of kernel for convolution\n",
    "\n",
    "# Hyperparameters to tune for specific case\n",
    "max_epoch_p = 500 # max number of epochs for pretraining\n",
    "max_epoch_f = 500 # max number of epochs for finetuning\n",
    "patience_p = 5 # number of consecutive rounds of early stopping condition before actual stop for pretraining\n",
    "patience_f = 10 # and finetuning\n",
    "tol_p = 1e-4 # minimum threshold for the difference between consecutive values of train rmse, used for early stopping, for pretraining\n",
    "tol_f = 1e-5 # and finetuning\n",
    "lambda_2 = 20. # regularisation of number or parameters\n",
    "lambda_s = 0.01 # regularisation of sparsity of the final matrix\n",
    "dot_scale = 1 # dot product weight for global kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matrix_Factorization(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_m, n_u, n_hid):\n",
    "        super().__init__()\n",
    "        # output (n_m, n_u)\n",
    "        self.X = nn.Parameter(torch.randn(n_m, n_hid)) # item\n",
    "        self.W = nn.Parameter(torch.randn(n_u, n_hid)) # user\n",
    "        \n",
    "        self.bm = nn.Parameter(torch.randn(n_m, 1))\n",
    "        self.bu = nn.Parameter(torch.randn(1, n_u))\n",
    "    \n",
    "    def forward(self):\n",
    "\n",
    "        pre = self.X.matmul(self.W.T) + self.bm + self.bu\n",
    "        return pre\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def forward(self, pred_p, reg_loss, train_m, train_r):\n",
    "      # L2 loss\n",
    "      diff = train_m * (train_r - pred_p)\n",
    "      sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff))\n",
    "      loss_p = sqE + reg_loss\n",
    "      return loss_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Matrix_Factorization(n_m, n_u, n_hid).to(device)\n",
    "model.load_state_dict(torch.load('matrix_factorization_best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_k(score_label, k):\n",
    "    dcg, i = 0., 0\n",
    "    for s in score_label:\n",
    "        if i < k:\n",
    "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
    "            i += 1\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_k(y_hat, y, k):\n",
    "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
    "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
    "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
    "    norm, i = 0., 0\n",
    "    for s in score_label_:\n",
    "        if i < k:\n",
    "            norm += (2**s[1]-1) / np.log2(2+i)\n",
    "            i += 1\n",
    "    dcg = dcg_k(score_label, k)\n",
    "    return dcg / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ndcg(y_hat, y):\n",
    "    ndcg_sum, num = 0, 0\n",
    "    y_hat, y = y_hat.T, y.T\n",
    "    n_users = y.shape[0]\n",
    "\n",
    "    for i in range(n_users):\n",
    "        y_hat_i = y_hat[i][np.where(y[i])]\n",
    "        y_i = y[i][np.where(y[i])]\n",
    "\n",
    "        if y_i.shape[0] < 2:\n",
    "            continue\n",
    "\n",
    "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
    "        num += 1\n",
    "\n",
    "    return ndcg_sum / num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_true_binary = (y_true >= 4).astype(int)\n",
    "    y_pred_binary = (y_pred >= 4).astype(int)\n",
    "    ps = precision_score(y_true_binary, y_pred_binary)\n",
    "    return ps\n",
    "def recall(y_true, y_pred):\n",
    "    y_true_binary = (y_true >= 4).astype(int)\n",
    "    y_pred_binary = (y_pred >= 4).astype(int)\n",
    "    rs = recall_score(y_true_binary, y_pred_binary)\n",
    "    return rs\n",
    "def f1(y_true, y_pred):\n",
    "    y_true_binary = (y_true >= 4).astype(int)\n",
    "    y_pred_binary = (y_pred >= 4).astype(int)\n",
    "    ps = f1_score(y_true_binary, y_pred_binary)\n",
    "    return ps\n",
    "def mae(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1., 5.)\n",
    "    res = mean_absolute_error(y_true, y_pred)\n",
    "    return res\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1., 5.)\n",
    "    res = mean_squared_error(y_true, y_pred)\n",
    "    return np.sqrt(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 300 val rmse: 2.2958633745877934 val mae: 2.771623465075052 val ndcg: 0.8478429442103421\n",
      "Epoch: 300 train rmse: 2.271706183799881 train mae: 1.9700929302830295 train ndcg: 0.8313156763362318\n",
      "Time: 1.8353769779205322 seconds\n",
      "Time cumulative: 1.8353769779205322 seconds\n",
      " best rmse: 2.2958633745877934\n",
      " best mae: 2.771623465075052\n",
      " best ndcg: 0.8478429442103421\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 350 val rmse: 2.294636691630281 val mae: 2.7724370703707417 val ndcg: 0.847189164242488\n",
      "Epoch: 350 train rmse: 2.190293239505432 train mae: 1.8563982015863134 train ndcg: 0.8409751289841386\n",
      "Time: 302.7968544960022 seconds\n",
      "Time cumulative: 8014.76665353775 seconds\n",
      " best rmse: 2.294636691630281\n",
      " best mae: 2.771623465075052\n",
      " best ndcg: 0.8481803306178561\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 400 val rmse: 2.2926450611576925 val mae: 2.7746768861586935 val ndcg: 0.8471797643172172\n",
      "Epoch: 400 train rmse: 2.090903425824113 train mae: 1.7255932252062949 train ndcg: 0.8542200812839125\n",
      "Time: 569.4600772857666 seconds\n",
      "Time cumulative: 30072.986295938492 seconds\n",
      " best rmse: 2.2926450611576925\n",
      " best mae: 2.771623465075052\n",
      " best ndcg: 0.8481803306178561\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "FINE-TUNING\n",
      "Epoch: 450 val rmse: 2.2900245739507303 val mae: 2.7782975554586193 val ndcg: 0.8470631478605682\n",
      "Epoch: 450 train rmse: 1.9694501824297312 train mae: 1.5778757252606037 train ndcg: 0.8706594382772469\n",
      "Time: 828.3111939430237 seconds\n",
      "Time cumulative: 65144.99285387993 seconds\n",
      " best rmse: 2.2900245739507303\n",
      " best mae: 2.771623465075052\n",
      " best ndcg: 0.8481803306178561\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
     ]
    }
   ],
   "source": [
    "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
    "best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
    "\n",
    "time_cumulative = 0\n",
    "tic = time()\n",
    "\n",
    "# Fine-Tuning\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=lambda_s)\n",
    "\n",
    "def closure():\n",
    "  optimizer.zero_grad()\n",
    "  x = torch.Tensor(train_r).double().to(device)\n",
    "  m = torch.Tensor(train_m).double().to(device)\n",
    "  model.train()\n",
    "  pred = model()\n",
    "  loss = Loss().to(device)(pred, 0, m, x)\n",
    "  loss.backward()\n",
    "  return loss\n",
    "\n",
    "last_rmse = np.inf\n",
    "counter = 0\n",
    "\n",
    "for i in range(300, max_epoch_f):\n",
    "  optimizer.step(closure)\n",
    "  model.eval()\n",
    "  t = time() - tic\n",
    "  time_cumulative += t\n",
    "\n",
    "  pre = model()\n",
    "  \n",
    "  pre = pre.float().cpu().detach().numpy()\n",
    "\n",
    "  error = (val_m * (np.clip(pre, 1., 5.) - val_r) ** 2).sum() / test_m.sum()  # test error\n",
    "  val_rmse = np.sqrt(error)\n",
    "\n",
    "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
    "  train_rmse = np.sqrt(error_train)\n",
    "\n",
    "  val_mae = (val_m * np.abs(np.clip(pre, 1., 5.) - test_r)).sum() / test_m.sum()\n",
    "  train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()\n",
    "\n",
    "  val_ndcg = call_ndcg(np.clip(pre, 1., 5.), test_r)\n",
    "  train_ndcg = call_ndcg(np.clip(pre, 1., 5.), train_r)\n",
    "\n",
    "  if val_rmse < best_rmse:\n",
    "      best_rmse = val_rmse\n",
    "      best_rmse_ep = i+1\n",
    "\n",
    "  if val_mae < best_mae:\n",
    "      best_mae = val_mae\n",
    "      torch.save(model.state_dict(), 'matrix_factorization_best.pth')\n",
    "      best_mae_ep = i+1\n",
    "\n",
    "  if best_ndcg < val_ndcg:\n",
    "      best_ndcg = val_ndcg\n",
    "      best_ndcg_ep = i+1\n",
    "\n",
    "  if last_rmse-train_rmse < tol_f:\n",
    "    counter += 1\n",
    "  else:\n",
    "    counter = 0\n",
    "\n",
    "  last_rmse = train_rmse\n",
    "\n",
    "  if patience_f == counter:\n",
    "    print('.-^-._' * 12)\n",
    "    print('FINE-TUNING')\n",
    "    print('Epoch:', i+1, 'val rmse:', val_rmse, 'val mae:', val_mae, 'val ndcg:', val_ndcg)\n",
    "    print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
    "    print('Time:', t, 'seconds')\n",
    "    print('Time cumulative:', time_cumulative, 'seconds')\n",
    "    print('.-^-._' * 12)\n",
    "    break\n",
    "\n",
    "\n",
    "  if i % 50 != 0:\n",
    "    continue\n",
    "\n",
    "  print('.-^-._' * 12)\n",
    "  print('FINE-TUNING')\n",
    "  print('Epoch:', i, 'val rmse:', val_rmse, 'val mae:', val_mae, 'val ndcg:', val_ndcg)\n",
    "  print('Epoch:', i, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
    "  print('Time:', t, 'seconds')\n",
    "  print('Time cumulative:', time_cumulative, 'seconds')\n",
    "  print(' best rmse:', best_rmse)\n",
    "  print(' best mae:', best_mae)\n",
    "  print(' best ndcg:', best_ndcg)\n",
    "  print('.-^-._' * 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500  best rmse: 2.2874108558515767\n",
      "Epoch: 301  best mae: 2.771623465075052\n",
      "Epoch: 319  best ndcg: 0.8481803306178561\n"
     ]
    }
   ],
   "source": [
    "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
    "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
    "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  discreted_link_ratings_p0.csv\n",
      "best_mae:  2.0008756559590974\n",
      "best_rmse:  2.2908428935320675\n",
      "best_precision:  0.4989463543861762\n",
      "best_recall:  0.4300467047223664\n",
      "best_f1:  0.4619415256835475\n"
     ]
    }
   ],
   "source": [
    "best_model = Matrix_Factorization(n_m, n_u, n_hid).to(device)\n",
    "best_model.load_state_dict(torch.load('matrix_factorization_best.pth'))\n",
    "pred = best_model()\n",
    "pred = pred.float().cpu().detach().numpy()\n",
    "y_pred = np.extract(test_m, pred)\n",
    "y_true = np.extract(test_m, test_r)\n",
    "\n",
    "best_mae = mae(y_true, y_pred)\n",
    "best_rmse = rmse(y_true, y_pred)\n",
    "best_precision = precision(y_true, y_pred)\n",
    "best_recall = recall(y_true, y_pred)\n",
    "best_f1 = f1(y_true, y_pred)\n",
    "\n",
    "print('data: ', data_path)\n",
    "print('best_mae: ', best_mae)\n",
    "print('best_rmse: ', best_rmse)\n",
    "print('best_precision: ', best_precision)\n",
    "print('best_recall: ', best_recall)\n",
    "print('best_f1: ', best_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
