{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fleanend/TorchGlocalK/blob/main/Original_Glocal_K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nl2tU6kL8Ot3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from time import time\n",
        "from scipy.sparse import csc_matrix\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.parameter import Parameter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error\n",
        "torch.manual_seed(1284)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Loader Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_from_csv(path='./', delimiter=',', max_user = None):\n",
        "    df = pd.read_csv(path, index_col=0)\n",
        "    print(df.columns)\n",
        "    rows_with_nan = df[df.isna().any(axis=1)]\n",
        "    print(rows_with_nan)\n",
        "    df = df[['userId', 'movieId', 'rating']]\n",
        "\n",
        "    # print( df['movieId'])\n",
        "    if max_user:\n",
        "        df = df.loc[df['userId'] <= max_user]\n",
        "    def discrete(column):\n",
        "        sorted_values = sorted(df[column].unique(), reverse=True)\n",
        "        def custom_rank(value):\n",
        "            return sorted_values.index(value) + 1\n",
        "        return custom_rank\n",
        "\n",
        "    # df['userId'] = df['userId'].apply(discrete('userId'))\n",
        "    # df['movieId'] = df['movieId'].apply(discrete('movieId'))\n",
        "    # df.to_csv('discreted_link_ratings.csv')\n",
        "    n_u = df['userId'].nunique()\n",
        "    n_m = df['movieId'].nunique()\n",
        "    # Chia tập train từ tập dữ liệu\n",
        "    train, df_temp = train_test_split(df, test_size=0.4, shuffle=True, random_state=42)\n",
        "\n",
        "    # Chia tập test và tập validation từ tập dữ liệu còn lại\n",
        "    test, val = train_test_split(df_temp, test_size=0.5, shuffle=True, random_state=42)\n",
        "    print(train.shape, val.shape, test.shape, n_m, n_u)\n",
        "    # print(train.iloc[0])\n",
        "    # print(df.iloc[0])\n",
        "    train_r = np.zeros((n_m, n_u))\n",
        "    val_r = np.zeros((n_m, n_u))\n",
        "    test_r = np.zeros((n_m, n_u))\n",
        "\n",
        "    n_train = train.shape[0]  # num of training ratings\n",
        "    n_val = val.shape[0]  # num of valid ratings\n",
        "    n_test = test.shape[0]  # num of test ratings\n",
        "\n",
        "    for i in range(n_train):\n",
        "        # print(train.iloc[i,1]-1, train.iloc[i,0]-1)\n",
        "        train_r[train.iloc[i,1]-1, train.iloc[i,0]-1] = train.iloc[i,2]\n",
        "\n",
        "    for i in range(n_val):\n",
        "        # print(train.iloc[i,1]-1, train.iloc[i,0]-1)\n",
        "        val_r[val.iloc[i,1]-1, val.iloc[i,0]-1] = val.iloc[i,2]\n",
        "\n",
        "    for i in range(n_test):\n",
        "        test_r[test.iloc[i,1]-1, test.iloc[i,0]-1] = test.iloc[i,2]\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    val_m = np.greater(val_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of valid ratings: {}'.format(n_val))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "    return n_m, n_u, train_r, train_m, val_r, val_m, test_r, test_m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0fkA1WpmipzF"
      },
      "outputs": [],
      "source": [
        "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "data_path = '../processed_data_for_matrix_completion/discreted_link_ratings_p0.csv'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJqSSY33mgkw",
        "outputId": "e11a106d-2231-428c-fb0a-48858167c55f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['userId', 'movieId', 'rating'], dtype='object')\n",
            "Empty DataFrame\n",
            "Columns: [userId, movieId, rating]\n",
            "Index: []\n",
            "(117680, 3) (39227, 3) (39227, 3) 14363 1999\n",
            "data matrix loaded\n",
            "num of users: 1999\n",
            "num of movies: 14363\n",
            "num of training ratings: 117680\n",
            "num of valid ratings: 39227\n",
            "num of test ratings: 39227\n"
          ]
        }
      ],
      "source": [
        "# Data Load\n",
        "path = data_path\n",
        "n_m, n_u, train_r, train_m, val_r, val_m, test_r, test_m = load_data_from_csv(path=path, delimiter='\\t')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nGCdp_FlobOK"
      },
      "outputs": [],
      "source": [
        "# Common hyperparameter settings\n",
        "n_hid = 500 # size of hidden layers\n",
        "n_dim = 5 # inner AE embedding size\n",
        "n_layers = 2 # number of hidden layers\n",
        "gk_size = 3 # width=height of kernel for convolution\n",
        "\n",
        "# Hyperparameters to tune for specific case\n",
        "max_epoch_p = 500 # max number of epochs for pretraining\n",
        "max_epoch_f = 1000 # max number of epochs for finetuning\n",
        "patience_p = 5 # number of consecutive rounds of early stopping condition before actual stop for pretraining\n",
        "patience_f = 10 # and finetuning\n",
        "tol_p = 1e-4 # minimum threshold for the difference between consecutive values of train rmse, used for early stopping, for pretraining\n",
        "tol_f = 1e-5 # and finetuning\n",
        "lambda_2 = 20. # regularisation of number or parameters\n",
        "lambda_s = 0.006 # regularisation of sparsity of the final matrix\n",
        "dot_scale = 1 # dot product weight for global kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sWtU4-pmDDT"
      },
      "source": [
        "# Network Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p1P6fgYiy28F"
      },
      "outputs": [],
      "source": [
        "def local_kernel(u, v):\n",
        "    dist = torch.norm(u - v, p=2, dim=2)\n",
        "    hat = torch.clamp(1. - dist**2, min=0.)\n",
        "    return hat\n",
        "\n",
        "class KernelLayer(nn.Module):\n",
        "    def __init__(self, n_in, n_hid, n_dim, lambda_s, lambda_2, activation=nn.Sigmoid()):\n",
        "      super().__init__()\n",
        "      self.W = nn.Parameter(torch.randn(n_in, n_hid))\n",
        "      self.u = nn.Parameter(torch.randn(n_in, 1, n_dim))\n",
        "      self.v = nn.Parameter(torch.randn(1, n_hid, n_dim))\n",
        "      self.b = nn.Parameter(torch.randn(n_hid))\n",
        "\n",
        "      self.lambda_s = lambda_s\n",
        "      self.lambda_2 = lambda_2\n",
        "\n",
        "      nn.init.xavier_uniform_(self.W, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.xavier_uniform_(self.u, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.xavier_uniform_(self.v, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.zeros_(self.b)\n",
        "      self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "      w_hat = local_kernel(self.u, self.v) # (n_in, n_hid)\n",
        "    \n",
        "      sparse_reg = torch.nn.functional.mse_loss(w_hat, torch.zeros_like(w_hat))\n",
        "      sparse_reg_term = self.lambda_s * sparse_reg\n",
        "      \n",
        "      l2_reg = torch.nn.functional.mse_loss(self.W, torch.zeros_like(self.W))\n",
        "      l2_reg_term = self.lambda_2 * l2_reg\n",
        "\n",
        "      W_eff = self.W * w_hat  # Local kernelised weight matrix\n",
        "      y = torch.matmul(x, W_eff) + self.b # (m, n_hid)\n",
        "      y = self.activation(y) \n",
        "\n",
        "      return y, sparse_reg_term + l2_reg_term\n",
        "\n",
        "class KernelNet(nn.Module):\n",
        "    def __init__(self, n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2):\n",
        "      super().__init__()\n",
        "      layers = []\n",
        "      for i in range(n_layers):\n",
        "        if i == 0:\n",
        "          layers.append(KernelLayer(n_u, n_hid, n_dim, lambda_s, lambda_2))\n",
        "        else:\n",
        "          layers.append(KernelLayer(n_hid, n_hid, n_dim, lambda_s, lambda_2))\n",
        "      layers.append(KernelLayer(n_hid, n_u, n_dim, lambda_s, lambda_2, activation=nn.Identity())) # output(m, n_u)\n",
        "      self.layers = nn.ModuleList(layers)\n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "      total_reg = None\n",
        "      for i, layer in enumerate(self.layers):\n",
        "        x, reg = layer(x)\n",
        "        if i < len(self.layers)-1:\n",
        "          x = self.dropout(x)\n",
        "        if total_reg is None:\n",
        "          total_reg = reg\n",
        "        else:\n",
        "          total_reg += reg\n",
        "      return x, total_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7RGKh1ckXgtP"
      },
      "outputs": [],
      "source": [
        "class CompleteNet(nn.Module):\n",
        "    def __init__(self, kernel_net, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale):\n",
        "      super().__init__()\n",
        "      self.gk_size = gk_size\n",
        "      self.dot_scale = dot_scale\n",
        "      self.local_kernel_net = kernel_net # first stage\n",
        "      self.global_kernel_net = KernelNet(n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2) # last stage\n",
        "      self.conv_kernel = torch.nn.Parameter(torch.randn(n_m, gk_size**2) * 0.1)\n",
        "      nn.init.xavier_uniform_(self.conv_kernel, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      \n",
        "    # reference to figure 1 in the paper\n",
        "    def forward(self, train_r):\n",
        "      x, _ = self.local_kernel_net(train_r) # pretrain with Local Kernel-base\n",
        "      gk = self.global_kernel(x, self.gk_size, self.dot_scale) # Create global kernel (GK) (equation 4, 5)\n",
        "      x = self.global_conv(train_r, gk) # convolution operation 𝑅ˆ = 𝑅 ⊗ 𝐺𝐾 equation 6\n",
        "      x, global_reg_loss = self.global_kernel_net(x) # fine tuning with Global Kernel-based Matrix\n",
        "      return x, global_reg_loss\n",
        "\n",
        "    def global_kernel(self, input, gk_size, dot_scale):\n",
        "      avg_pooling = torch.mean(input, dim=1)  # Item (axis=1) based average pooling\n",
        "      avg_pooling = avg_pooling.view(1, -1)\n",
        "\n",
        "      gk = torch.matmul(avg_pooling, self.conv_kernel) * dot_scale  # Scaled dot product\n",
        "      gk = gk.view(1, 1, gk_size, gk_size)\n",
        "\n",
        "      return gk\n",
        "\n",
        "    def global_conv(self, input, W):\n",
        "      input = input.unsqueeze(0).unsqueeze(0)\n",
        "      conv2d = nn.LeakyReLU()(F.conv2d(input, W, stride=1, padding=1))\n",
        "      return conv2d.squeeze(0).squeeze(0)\n",
        "\n",
        "class Loss(nn.Module):\n",
        "    def forward(self, pred_p, reg_loss, train_m, train_r):\n",
        "      # L2 loss\n",
        "      diff = train_m * (train_r - pred_p)\n",
        "      sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff))\n",
        "      loss_p = sqE + reg_loss\n",
        "      return loss_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8sQCwrSmKG4"
      },
      "source": [
        "# Network Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtWj1SCo1RW"
      },
      "source": [
        "## Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7teUrgWagpW0"
      },
      "outputs": [],
      "source": [
        "model = KernelNet(n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2).double().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IEBsNhNo4Cj"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OiTXqnN6zLXQ"
      },
      "outputs": [],
      "source": [
        "complete_model = CompleteNet(model, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale).double().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sETwz58aK6y6"
      },
      "source": [
        "# Evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vyReXxgac3KH"
      },
      "outputs": [],
      "source": [
        "def dcg_k(score_label, k):\n",
        "    dcg, i = 0., 0\n",
        "    for s in score_label:\n",
        "        if i < k:\n",
        "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    return dcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jwsSR-8ZdGWo"
      },
      "outputs": [],
      "source": [
        "def ndcg_k(y_hat, y, k):\n",
        "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
        "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
        "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
        "    norm, i = 0., 0\n",
        "    for s in score_label_:\n",
        "        if i < k:\n",
        "            norm += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    dcg = dcg_k(score_label, k)\n",
        "    return dcg / norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yy9eQS51pbhj"
      },
      "outputs": [],
      "source": [
        "def call_ndcg(y_hat, y):\n",
        "    ndcg_sum, num = 0, 0\n",
        "    y_hat, y = y_hat.T, y.T\n",
        "    n_users = y.shape[0]\n",
        "\n",
        "    for i in range(n_users):\n",
        "        y_hat_i = y_hat[i][np.where(y[i])]\n",
        "        y_i = y[i][np.where(y[i])]\n",
        "\n",
        "        if y_i.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
        "        num += 1\n",
        "\n",
        "    return ndcg_sum / num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXQjeMxmYEC"
      },
      "source": [
        "# Training and Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ35Zoha-Eue",
        "outputId": "3276ea24-cc36-4ebc-ee86-966d30b0695c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 0 val rmse: 2.6424843061107093 train rmse: 2.6414038224585976\n",
            "Time: 4.226903200149536 seconds\n",
            "Time cumulative: 4.226903200149536 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 28 val rmse: 1.1277923592997117 train rmse: 1.103697659503925\n",
            "Time: 91.32204341888428 seconds\n",
            "Time cumulative: 1317.8092114925385 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ],
      "source": [
        "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
        "best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
        "\n",
        "time_cumulative = 0\n",
        "tic = time()\n",
        "\n",
        "# Pre-Training\n",
        "optimizer = torch.optim.AdamW(complete_model.local_kernel_net.parameters(), lr=0.001)\n",
        "\n",
        "def closure():\n",
        "  optimizer.zero_grad()\n",
        "  x = torch.Tensor(train_r).double().to(device)\n",
        "  m = torch.Tensor(train_m).double().to(device)\n",
        "  complete_model.local_kernel_net.train()\n",
        "  pred, reg = complete_model.local_kernel_net(x)\n",
        "  loss = Loss().to(device)(pred, reg, m, x)\n",
        "  loss.backward()\n",
        "  return loss\n",
        "\n",
        "last_rmse = np.inf\n",
        "counter = 0\n",
        "\n",
        "for i in range(max_epoch_p):\n",
        "  optimizer.step(closure)\n",
        "  complete_model.local_kernel_net.eval()\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "\n",
        "  pre, _ = model(torch.Tensor(train_r).double().to(device))\n",
        "  \n",
        "  pre = pre.float().cpu().detach().numpy()\n",
        "  \n",
        "  error = (val_m * (np.clip(pre, 1., 5.) - val_r) ** 2).sum() / val_m.sum()  # val error\n",
        "  val_rmse = np.sqrt(error)\n",
        "\n",
        "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train)\n",
        "\n",
        "  if last_rmse-train_rmse < tol_p:\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter = 0\n",
        "\n",
        "  last_rmse = train_rmse\n",
        "\n",
        "  if patience_p == counter:\n",
        "    print('.-^-._' * 12)\n",
        "    print('PRE-TRAINING')\n",
        "    print('Epoch:', i+1, 'val rmse:', val_rmse, 'train rmse:', train_rmse)\n",
        "    print('Time:', t, 'seconds')\n",
        "    print('Time cumulative:', time_cumulative, 'seconds')\n",
        "    print('.-^-._' * 12)\n",
        "    break\n",
        "\n",
        "\n",
        "  if i % 50 != 0:\n",
        "    continue\n",
        "  print('.-^-._' * 12)\n",
        "  print('PRE-TRAINING')\n",
        "  print('Epoch:', i, 'val rmse:', val_rmse, 'train rmse:', train_rmse)\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6v_tODcweLn",
        "outputId": "981655be-0c5c-4d7a-c8ad-ea389086451c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 0 val rmse: 2.63881348395579 val mae: 2.4330083762695467 val ndcg: 0.8494356085642428\n",
            "Epoch: 0 train rmse: 2.63797145434049 train mae: 2.430917803688232 train ndcg: 0.8318347764663317\n",
            "Time: 97.7194721698761 seconds\n",
            "Time cumulative: 1415.5286836624146 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 50 val rmse: 1.016921407175779 val mae: 0.7968755306391725 val ndcg: 0.885078461644091\n",
            "Epoch: 50 train rmse: 0.9880486897616867 train mae: 0.7780053150406863 train ndcg: 0.8749428296174129\n",
            "Time: 559.75257396698 seconds\n",
            "Time cumulative: 18072.58071756363 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 100 val rmse: 0.9533205891607958 val mae: 0.7456952926327778 val ndcg: 0.8909793279183733\n",
            "Epoch: 100 train rmse: 0.9276883162042374 train mae: 0.7287246761047722 train ndcg: 0.8797460089558536\n",
            "Time: 1028.9960033893585 seconds\n",
            "Time cumulative: 58055.12632250786 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 109 val rmse: 0.9553345386556533 val mae: 0.7472727436725956 val ndcg: 0.8918576122627203\n",
            "Epoch: 109 train rmse: 0.9290679204498373 train mae: 0.7300398078621034 train ndcg: 0.8790599976404224\n",
            "Time: 1104.3629138469696 seconds\n",
            "Time cumulative: 66627.05586194992 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ],
      "source": [
        "# Fine-Tuning\n",
        "optimizer = torch.optim.AdamW(complete_model.parameters(), lr=0.001)\n",
        "\n",
        "def closure():\n",
        "  optimizer.zero_grad()\n",
        "  x = torch.Tensor(train_r).double().to(device)\n",
        "  m = torch.Tensor(train_m).double().to(device)\n",
        "  complete_model.train()\n",
        "  pred, reg = complete_model(x)\n",
        "  loss = Loss().to(device)(pred, reg, m, x)\n",
        "  loss.backward()\n",
        "  return loss\n",
        "\n",
        "last_rmse = np.inf\n",
        "counter = 0\n",
        "\n",
        "for i in range(max_epoch_f):\n",
        "  optimizer.step(closure)\n",
        "  complete_model.eval()\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "\n",
        "  pre, _ = complete_model(torch.Tensor(train_r).double().to(device))\n",
        "  \n",
        "  pre = pre.float().cpu().detach().numpy()\n",
        "\n",
        "  error = (val_m * (np.clip(pre, 1., 5.) - val_r) ** 2).sum() / val_m.sum()  # val error\n",
        "  val_rmse = np.sqrt(error)\n",
        "\n",
        "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train)\n",
        "\n",
        "  val_mae = (val_m * np.abs(np.clip(pre, 1., 5.) - val_r)).sum() / val_m.sum()\n",
        "  train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()\n",
        "\n",
        "  val_ndcg = call_ndcg(np.clip(pre, 1., 5.), val_r)\n",
        "  train_ndcg = call_ndcg(np.clip(pre, 1., 5.), train_r)\n",
        "\n",
        "  if val_rmse < best_rmse:\n",
        "      best_rmse = val_rmse\n",
        "      best_rmse_ep = i+1\n",
        "\n",
        "  if val_mae < best_mae:\n",
        "      best_mae = val_mae\n",
        "      torch.save(complete_model.state_dict(), 'Glocal-K_best.pth')\n",
        "      best_mae_ep = i+1\n",
        "\n",
        "  if best_ndcg < val_ndcg:\n",
        "      best_ndcg = val_ndcg\n",
        "      best_ndcg_ep = i+1\n",
        "\n",
        "  if last_rmse-train_rmse < tol_f:\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter = 0\n",
        "\n",
        "  last_rmse = train_rmse\n",
        "\n",
        "  if patience_f == counter:\n",
        "    print('.-^-._' * 12)\n",
        "    print('FINE-TUNING')\n",
        "    print('Epoch:', i+1, 'val rmse:', val_rmse, 'val mae:', val_mae, 'val ndcg:', val_ndcg)\n",
        "    print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "    print('Time:', t, 'seconds')\n",
        "    print('Time cumulative:', time_cumulative, 'seconds')\n",
        "    print('.-^-._' * 12)\n",
        "    break\n",
        "\n",
        "\n",
        "  if i % 50 != 0:\n",
        "    continue\n",
        "\n",
        "  print('.-^-._' * 12)\n",
        "  print('FINE-TUNING')\n",
        "  print('Epoch:', i, 'val rmse:', val_rmse, 'val mae:', val_mae, 'val ndcg:', val_ndcg)\n",
        "  print('Epoch:', i, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTi_PdXJqTjh",
        "outputId": "6eb34426-83f7-4c3e-ea2e-d9c1f3df1a34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 99  best rmse: 0.9528607062081208\n",
            "Epoch: 99  best mae: 0.7452099026218637\n",
            "Epoch: 105  best ndcg: 0.8926667333490804\n"
          ]
        }
      ],
      "source": [
        "# Final result\n",
        "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
        "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
        "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precision(y_true, y_pred):\n",
        "    y_true_binary = (y_true >= 4).astype(int)\n",
        "    y_pred_binary = (y_pred >= 4).astype(int)\n",
        "    res = precision_score(y_true_binary, y_pred_binary)\n",
        "    return res\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    y_true_binary = (y_true >= 4).astype(int)\n",
        "    y_pred_binary = (y_pred >= 4).astype(int)\n",
        "    res = recall_score(y_true_binary, y_pred_binary)\n",
        "    return res\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_true_binary = (y_true >= 4).astype(int)\n",
        "    y_pred_binary = (y_pred >= 4).astype(int)\n",
        "    res = f1_score(y_true_binary, y_pred_binary)\n",
        "    return res\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1., 5.)\n",
        "    res = mean_absolute_error(y_true, y_pred)\n",
        "    return res\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1., 5.)\n",
        "    res = mean_squared_error(y_true, y_pred)\n",
        "    return np.sqrt(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b6Yfh3hm4Efa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data:  ../processed_data_for_matrix_completion/discreted_link_ratings_p0.csv\n",
            "best_mae:  0.8111106496160975\n",
            "best_rmse:  1.0239966646299297\n",
            "best_precision:  0.752067963335569\n",
            "best_recall:  0.17457187337830826\n",
            "best_f1:  0.2833677294360443\n"
          ]
        }
      ],
      "source": [
        "best_model = CompleteNet(model, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale).double().to(device)\n",
        "best_model.load_state_dict(torch.load('Glocal-K_best.pth'))\n",
        "pred, _ = best_model(torch.Tensor(train_r).double().to(device))\n",
        "pred = pred.float().cpu().detach().numpy()\n",
        "y_pred = np.extract(test_m, pred)\n",
        "y_true = np.extract(test_m, test_r)\n",
        "\n",
        "best_mae = mae(y_true, y_pred)\n",
        "best_rmse = rmse(y_true, y_pred)\n",
        "best_precision = precision(y_true, y_pred)\n",
        "best_recall = recall(y_true, y_pred)\n",
        "best_f1 = f1(y_true, y_pred)\n",
        "\n",
        "print('data: ', data_path)\n",
        "print('best_mae: ', best_mae)\n",
        "print('best_rmse: ', best_rmse)\n",
        "print('best_precision: ', best_precision)\n",
        "print('best_recall: ', best_recall)\n",
        "print('best_f1: ', best_f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
