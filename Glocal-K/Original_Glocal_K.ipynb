{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fleanend/TorchGlocalK/blob/main/Original_Glocal_K.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nl2tU6kL8Ot3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from time import time\n",
        "from scipy.sparse import csc_matrix\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.parameter import Parameter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error\n",
        "torch.manual_seed(1284)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Loader Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cq3KEUaVo1o3"
      },
      "outputs": [],
      "source": [
        "def load_data_100k(path='./', delimiter='\\t'):\n",
        "\n",
        "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    total = np.concatenate((train, test), axis=0)\n",
        "\n",
        "    n_u = np.unique(total[:,0]).size  # num of users\n",
        "    n_m = np.unique(total[:,1]).size  # num of movies\n",
        "    n_train = train.shape[0]  # num of training ratings\n",
        "    n_test = test.shape[0]  # num of test ratings\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_train):\n",
        "        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n",
        "\n",
        "    for i in range(n_test):\n",
        "        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_from_csv(path='./', delimiter=',', max_user = None):\n",
        "    df = pd.read_csv(path, index_col=0)\n",
        "    print(df.columns)\n",
        "    rows_with_nan = df[df.isna().any(axis=1)]\n",
        "    print(rows_with_nan)\n",
        "    df = df[['userId', 'movieId', 'rating']]\n",
        "\n",
        "    # print( df['movieId'])\n",
        "    if max_user:\n",
        "        df = df.loc[df['userId'] <= max_user]\n",
        "    def discrete(column):\n",
        "        sorted_values = sorted(df[column].unique(), reverse=True)\n",
        "        def custom_rank(value):\n",
        "            return sorted_values.index(value) + 1\n",
        "        return custom_rank\n",
        "\n",
        "    df['userId'] = df['userId'].apply(discrete('userId'))\n",
        "    df['movieId'] = df['movieId'].apply(discrete('movieId'))\n",
        "    # df.to_csv('discreted_link_ratings.csv')\n",
        "    n_u = df['userId'].nunique()\n",
        "    n_m = df['movieId'].nunique()\n",
        "    train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n",
        "    print(train.shape, test.shape, n_m, n_u)\n",
        "    # print(train.iloc[0])\n",
        "    # print(df.iloc[0])\n",
        "    train_r = np.zeros((n_m, n_u))\n",
        "    test_r = np.zeros((n_m, n_u))\n",
        "\n",
        "    n_train = train.shape[0]  # num of training ratings\n",
        "    n_test = test.shape[0]  # num of test ratings\n",
        "\n",
        "    for i in range(n_train):\n",
        "        # print(train.iloc[i,1]-1, train.iloc[i,0]-1)\n",
        "        train_r[train.iloc[i,1]-1, train.iloc[i,0]-1] = train.iloc[i,2]\n",
        "\n",
        "    for i in range(n_test):\n",
        "        test_r[test.iloc[i,1]-1, test.iloc[i,0]-1] = test.iloc[i,2]\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "0fkA1WpmipzF"
      },
      "outputs": [],
      "source": [
        "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "data_path = '../processed_data_for_matrix_completion/discreted_link_ratings_p0.csv'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJqSSY33mgkw",
        "outputId": "e11a106d-2231-428c-fb0a-48858167c55f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0', 'userId', 'movieId', 'rating'], dtype='object')\n",
            "Empty DataFrame\n",
            "Columns: [Unnamed: 0, userId, movieId, rating]\n",
            "Index: []\n",
            "(78223, 3) (19556, 3) 9160 999\n",
            "data matrix loaded\n",
            "num of users: 999\n",
            "num of movies: 9160\n",
            "num of training ratings: 78223\n",
            "num of test ratings: 19556\n"
          ]
        }
      ],
      "source": [
        "# Data Load\n",
        "path = data_path\n",
        "n_m, n_u, train_r, train_m, test_r, test_m = load_data_from_csv(path=path, delimiter='\\t')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nGCdp_FlobOK"
      },
      "outputs": [],
      "source": [
        "# Common hyperparameter settings\n",
        "n_hid = 500 # size of hidden layers\n",
        "n_dim = 5 # inner AE embedding size\n",
        "n_layers = 2 # number of hidden layers\n",
        "gk_size = 3 # width=height of kernel for convolution\n",
        "\n",
        "# Hyperparameters to tune for specific case\n",
        "max_epoch_p = 500 # max number of epochs for pretraining\n",
        "max_epoch_f = 1000 # max number of epochs for finetuning\n",
        "patience_p = 5 # number of consecutive rounds of early stopping condition before actual stop for pretraining\n",
        "patience_f = 10 # and finetuning\n",
        "tol_p = 1e-4 # minimum threshold for the difference between consecutive values of train rmse, used for early stopping, for pretraining\n",
        "tol_f = 1e-5 # and finetuning\n",
        "lambda_2 = 20. # regularisation of number or parameters\n",
        "lambda_s = 0.006 # regularisation of sparsity of the final matrix\n",
        "dot_scale = 1 # dot product weight for global kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sWtU4-pmDDT"
      },
      "source": [
        "# Network Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "p1P6fgYiy28F"
      },
      "outputs": [],
      "source": [
        "def local_kernel(u, v):\n",
        "    dist = torch.norm(u - v, p=2, dim=2)\n",
        "    hat = torch.clamp(1. - dist**2, min=0.)\n",
        "    return hat\n",
        "\n",
        "class KernelLayer(nn.Module):\n",
        "    def __init__(self, n_in, n_hid, n_dim, lambda_s, lambda_2, activation=nn.Sigmoid()):\n",
        "      super().__init__()\n",
        "      self.W = nn.Parameter(torch.randn(n_in, n_hid))\n",
        "      self.u = nn.Parameter(torch.randn(n_in, 1, n_dim))\n",
        "      self.v = nn.Parameter(torch.randn(1, n_hid, n_dim))\n",
        "      self.b = nn.Parameter(torch.randn(n_hid))\n",
        "\n",
        "      self.lambda_s = lambda_s\n",
        "      self.lambda_2 = lambda_2\n",
        "\n",
        "      nn.init.xavier_uniform_(self.W, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.xavier_uniform_(self.u, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.xavier_uniform_(self.v, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.zeros_(self.b)\n",
        "      self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "      w_hat = local_kernel(self.u, self.v) # (n_in, n_hid)\n",
        "    \n",
        "      sparse_reg = torch.nn.functional.mse_loss(w_hat, torch.zeros_like(w_hat))\n",
        "      sparse_reg_term = self.lambda_s * sparse_reg\n",
        "      \n",
        "      l2_reg = torch.nn.functional.mse_loss(self.W, torch.zeros_like(self.W))\n",
        "      l2_reg_term = self.lambda_2 * l2_reg\n",
        "\n",
        "      W_eff = self.W * w_hat  # Local kernelised weight matrix\n",
        "      y = torch.matmul(x, W_eff) + self.b # (m, n_hid)\n",
        "      y = self.activation(y) \n",
        "\n",
        "      return y, sparse_reg_term + l2_reg_term\n",
        "\n",
        "class KernelNet(nn.Module):\n",
        "    def __init__(self, n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2):\n",
        "      super().__init__()\n",
        "      layers = []\n",
        "      for i in range(n_layers):\n",
        "        if i == 0:\n",
        "          layers.append(KernelLayer(n_u, n_hid, n_dim, lambda_s, lambda_2))\n",
        "        else:\n",
        "          layers.append(KernelLayer(n_hid, n_hid, n_dim, lambda_s, lambda_2))\n",
        "      layers.append(KernelLayer(n_hid, n_u, n_dim, lambda_s, lambda_2, activation=nn.Identity())) # output(m, n_u)\n",
        "      self.layers = nn.ModuleList(layers)\n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "      total_reg = None\n",
        "      for i, layer in enumerate(self.layers):\n",
        "        x, reg = layer(x)\n",
        "        if i < len(self.layers)-1:\n",
        "          x = self.dropout(x)\n",
        "        if total_reg is None:\n",
        "          total_reg = reg\n",
        "        else:\n",
        "          total_reg += reg\n",
        "      return x, total_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "7RGKh1ckXgtP"
      },
      "outputs": [],
      "source": [
        "class CompleteNet(nn.Module):\n",
        "    def __init__(self, kernel_net, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale):\n",
        "      super().__init__()\n",
        "      self.gk_size = gk_size\n",
        "      self.dot_scale = dot_scale\n",
        "      self.local_kernel_net = kernel_net # first stage\n",
        "      self.global_kernel_net = KernelNet(n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2) # last stage\n",
        "      self.conv_kernel = torch.nn.Parameter(torch.randn(n_m, gk_size**2) * 0.1)\n",
        "      nn.init.xavier_uniform_(self.conv_kernel, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      \n",
        "    # reference to figure 1 in the paper\n",
        "    def forward(self, train_r):\n",
        "      x, _ = self.local_kernel_net(train_r) # pretrain with Local Kernel-base\n",
        "      gk = self.global_kernel(x, self.gk_size, self.dot_scale) # Create global kernel (GK) (equation 4, 5)\n",
        "      x = self.global_conv(train_r, gk) # convolution operation 𝑅ˆ = 𝑅 ⊗ 𝐺𝐾 equation 6\n",
        "      x, global_reg_loss = self.global_kernel_net(x) # fine tuning with Global Kernel-based Matrix\n",
        "      return x, global_reg_loss\n",
        "\n",
        "    def global_kernel(self, input, gk_size, dot_scale):\n",
        "      avg_pooling = torch.mean(input, dim=1)  # Item (axis=1) based average pooling\n",
        "      avg_pooling = avg_pooling.view(1, -1)\n",
        "\n",
        "      gk = torch.matmul(avg_pooling, self.conv_kernel) * dot_scale  # Scaled dot product\n",
        "      gk = gk.view(1, 1, gk_size, gk_size)\n",
        "\n",
        "      return gk\n",
        "\n",
        "    def global_conv(self, input, W):\n",
        "      input = input.unsqueeze(0).unsqueeze(0)\n",
        "      conv2d = nn.LeakyReLU()(F.conv2d(input, W, stride=1, padding=1))\n",
        "      return conv2d.squeeze(0).squeeze(0)\n",
        "\n",
        "class Loss(nn.Module):\n",
        "    def forward(self, pred_p, reg_loss, train_m, train_r):\n",
        "      # L2 loss\n",
        "      diff = train_m * (train_r - pred_p)\n",
        "      sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff))\n",
        "      loss_p = sqE + reg_loss\n",
        "      return loss_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8sQCwrSmKG4"
      },
      "source": [
        "# Network Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtWj1SCo1RW"
      },
      "source": [
        "## Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "7teUrgWagpW0"
      },
      "outputs": [],
      "source": [
        "model = KernelNet(n_u, n_hid, n_dim, n_layers, lambda_s, lambda_2).double().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IEBsNhNo4Cj"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "OiTXqnN6zLXQ"
      },
      "outputs": [],
      "source": [
        "complete_model = CompleteNet(model, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale).double().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sETwz58aK6y6"
      },
      "source": [
        "# Evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "vyReXxgac3KH"
      },
      "outputs": [],
      "source": [
        "def dcg_k(score_label, k):\n",
        "    dcg, i = 0., 0\n",
        "    for s in score_label:\n",
        "        if i < k:\n",
        "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    return dcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "jwsSR-8ZdGWo"
      },
      "outputs": [],
      "source": [
        "def ndcg_k(y_hat, y, k):\n",
        "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
        "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
        "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
        "    norm, i = 0., 0\n",
        "    for s in score_label_:\n",
        "        if i < k:\n",
        "            norm += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    dcg = dcg_k(score_label, k)\n",
        "    return dcg / norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "yy9eQS51pbhj"
      },
      "outputs": [],
      "source": [
        "def call_ndcg(y_hat, y):\n",
        "    ndcg_sum, num = 0, 0\n",
        "    y_hat, y = y_hat.T, y.T\n",
        "    n_users = y.shape[0]\n",
        "\n",
        "    for i in range(n_users):\n",
        "        y_hat_i = y_hat[i][np.where(y[i])]\n",
        "        y_i = y[i][np.where(y[i])]\n",
        "\n",
        "        if y_i.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
        "        num += 1\n",
        "\n",
        "    return ndcg_sum / num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXQjeMxmYEC"
      },
      "source": [
        "# Training and Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ35Zoha-Eue",
        "outputId": "3276ea24-cc36-4ebc-ee86-966d30b0695c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 0 test rmse: 2.629006579937757 train rmse: 2.635278786576979\n",
            "Time: 0.15999913215637207 seconds\n",
            "Time cumulative: 0.15999913215637207 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 28 test rmse: 1.0984905472046014 train rmse: 1.0730708272131408\n",
            "Time: 32.09160566329956 seconds\n",
            "Time cumulative: 453.9142155647278 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ],
      "source": [
        "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
        "best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
        "\n",
        "time_cumulative = 0\n",
        "tic = time()\n",
        "\n",
        "# Pre-Training\n",
        "optimizer = torch.optim.AdamW(complete_model.local_kernel_net.parameters(), lr=0.001)\n",
        "\n",
        "def closure():\n",
        "  optimizer.zero_grad()\n",
        "  x = torch.Tensor(train_r).double().to(device)\n",
        "  m = torch.Tensor(train_m).double().to(device)\n",
        "  complete_model.local_kernel_net.train()\n",
        "  pred, reg = complete_model.local_kernel_net(x)\n",
        "  loss = Loss().to(device)(pred, reg, m, x)\n",
        "  loss.backward()\n",
        "  return loss\n",
        "\n",
        "last_rmse = np.inf\n",
        "counter = 0\n",
        "\n",
        "for i in range(max_epoch_p):\n",
        "  optimizer.step(closure)\n",
        "  complete_model.local_kernel_net.eval()\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "\n",
        "  pre, _ = model(torch.Tensor(train_r).double().to(device))\n",
        "  \n",
        "  pre = pre.float().cpu().detach().numpy()\n",
        "  \n",
        "  error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "  test_rmse = np.sqrt(error)\n",
        "\n",
        "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train)\n",
        "\n",
        "  if last_rmse-train_rmse < tol_p:\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter = 0\n",
        "\n",
        "  last_rmse = train_rmse\n",
        "\n",
        "  if patience_p == counter:\n",
        "    print('.-^-._' * 12)\n",
        "    print('PRE-TRAINING')\n",
        "    print('Epoch:', i+1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
        "    print('Time:', t, 'seconds')\n",
        "    print('Time cumulative:', time_cumulative, 'seconds')\n",
        "    print('.-^-._' * 12)\n",
        "    break\n",
        "\n",
        "\n",
        "  if i % 50 != 0:\n",
        "    continue\n",
        "  print('.-^-._' * 12)\n",
        "  print('PRE-TRAINING')\n",
        "  print('Epoch:', i, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6v_tODcweLn",
        "outputId": "981655be-0c5c-4d7a-c8ad-ea389086451c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 0 test rmse: 2.6388081145393554 test mae: 2.4223325226112133 test ndcg: 0.8469473003860399\n",
            "Epoch: 0 train rmse: 2.6440260629182184 train mae: 2.429025194241198 train ndcg: 0.8263251725108182\n",
            "Time: 34.31060576438904 seconds\n",
            "Time cumulative: 488.2248213291168 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 50 test rmse: 1.0074027068345663 test mae: 0.7973533520245124 test ndcg: 0.886821403158966\n",
            "Epoch: 50 train rmse: 0.9687280405824713 train mae: 0.7682230131149416 train ndcg: 0.8817748079160045\n",
            "Time: 194.90059518814087 seconds\n",
            "Time cumulative: 6293.383270740509 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 100 test rmse: 0.9506735637599414 test mae: 0.7516412125945054 test ndcg: 0.8930052245362443\n",
            "Epoch: 100 train rmse: 0.9098861701426086 train mae: 0.7207813567290766 train ndcg: 0.8805321174655818\n",
            "Time: 353.52852487564087 seconds\n",
            "Time cumulative: 20057.306021928787 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 145 test rmse: 0.9481961640293336 test mae: 0.7490779423990886 test ndcg: 0.8929940695050763\n",
            "Epoch: 145 train rmse: 0.9103091641479638 train mae: 0.7207897948928053 train ndcg: 0.8791689036156356\n",
            "Time: 494.0945825576782 seconds\n",
            "Time cumulative: 38770.10193324089 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ],
      "source": [
        "# Fine-Tuning\n",
        "optimizer = torch.optim.AdamW(complete_model.parameters(), lr=0.001)\n",
        "\n",
        "def closure():\n",
        "  optimizer.zero_grad()\n",
        "  x = torch.Tensor(train_r).double().to(device)\n",
        "  m = torch.Tensor(train_m).double().to(device)\n",
        "  complete_model.train()\n",
        "  pred, reg = complete_model(x)\n",
        "  loss = Loss().to(device)(pred, reg, m, x)\n",
        "  loss.backward()\n",
        "  return loss\n",
        "\n",
        "last_rmse = np.inf\n",
        "counter = 0\n",
        "\n",
        "for i in range(max_epoch_f):\n",
        "  optimizer.step(closure)\n",
        "  complete_model.eval()\n",
        "  t = time() - tic\n",
        "  time_cumulative += t\n",
        "\n",
        "  pre, _ = complete_model(torch.Tensor(train_r).double().to(device))\n",
        "  \n",
        "  pre = pre.float().cpu().detach().numpy()\n",
        "\n",
        "  error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "  test_rmse = np.sqrt(error)\n",
        "\n",
        "  error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train)\n",
        "\n",
        "  test_mae = (test_m * np.abs(np.clip(pre, 1., 5.) - test_r)).sum() / test_m.sum()\n",
        "  train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()\n",
        "\n",
        "  test_ndcg = call_ndcg(np.clip(pre, 1., 5.), test_r)\n",
        "  train_ndcg = call_ndcg(np.clip(pre, 1., 5.), train_r)\n",
        "\n",
        "  if test_rmse < best_rmse:\n",
        "      best_rmse = test_rmse\n",
        "      best_rmse_ep = i+1\n",
        "\n",
        "  if test_mae < best_mae:\n",
        "      best_mae = test_mae\n",
        "      torch.save(complete_model.state_dict(), 'Glocal-K_best.pth')\n",
        "      best_mae_ep = i+1\n",
        "\n",
        "  if best_ndcg < test_ndcg:\n",
        "      best_ndcg = test_ndcg\n",
        "      best_ndcg_ep = i+1\n",
        "\n",
        "  if last_rmse-train_rmse < tol_f:\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter = 0\n",
        "\n",
        "  last_rmse = train_rmse\n",
        "\n",
        "  if patience_f == counter:\n",
        "    print('.-^-._' * 12)\n",
        "    print('FINE-TUNING')\n",
        "    print('Epoch:', i+1, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
        "    print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "    print('Time:', t, 'seconds')\n",
        "    print('Time cumulative:', time_cumulative, 'seconds')\n",
        "    print('.-^-._' * 12)\n",
        "    break\n",
        "\n",
        "\n",
        "  if i % 50 != 0:\n",
        "    continue\n",
        "\n",
        "  print('.-^-._' * 12)\n",
        "  print('FINE-TUNING')\n",
        "  print('Epoch:', i, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
        "  print('Epoch:', i, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "  print('Time:', t, 'seconds')\n",
        "  print('Time cumulative:', time_cumulative, 'seconds')\n",
        "  print('.-^-._' * 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTi_PdXJqTjh",
        "outputId": "6eb34426-83f7-4c3e-ea2e-d9c1f3df1a34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 123  best rmse: 0.9461029128364461\n",
            "Epoch: 124  best mae: 0.7473415684244609\n",
            "Epoch: 125  best ndcg: 0.8948843010677874\n"
          ]
        }
      ],
      "source": [
        "# Final result\n",
        "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
        "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
        "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precision(y_true, y_pred):\n",
        "    y_true_binary = (y_true >= 4).astype(int)\n",
        "    y_pred_binary = (y_pred >= 4).astype(int)\n",
        "    res = precision_score(y_true_binary, y_pred_binary)\n",
        "    return res\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    y_true_binary = (y_true >= 4).astype(int)\n",
        "    y_pred_binary = (y_pred >= 4).astype(int)\n",
        "    res = recall_score(y_true_binary, y_pred_binary)\n",
        "    return res\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_true_binary = (y_true >= 4).astype(int)\n",
        "    y_pred_binary = (y_pred >= 4).astype(int)\n",
        "    res = f1_score(y_true_binary, y_pred_binary)\n",
        "    return res\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1., 5.)\n",
        "    res = mean_absolute_error(y_true, y_pred)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b6Yfh3hm4Efa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data:  ../processed_data_for_matrix_completion/discreted_link_ratings_p0.csv\n",
            "best_mae:  2.3828901315445745\n",
            "best_precision:  0.0\n",
            "best_recall:  0.0\n",
            "best_f1:  0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ABC\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "best_model = CompleteNet(model, n_u, n_m, n_hid, n_dim, n_layers, lambda_s, lambda_2, gk_size, dot_scale).double().to(device)\n",
        "# best_model.load_state_dict(torch.load('Glocal-K_best.pth'))\n",
        "pred, _ = best_model(torch.Tensor(train_r).double().to(device))\n",
        "pred = pred.float().cpu().detach().numpy()\n",
        "y_pred = np.extract(test_m, pred)\n",
        "y_true = np.extract(test_m, test_r)\n",
        "\n",
        "best_mae = mae(y_true, y_pred)\n",
        "best_precision = precision(y_true, y_pred)\n",
        "best_recall = recall(y_true, y_pred)\n",
        "best_f1 = f1(y_true, y_pred)\n",
        "\n",
        "print('data: ', data_path)\n",
        "print('best_mae: ', best_mae)\n",
        "print('best_precision: ', best_precision)\n",
        "print('best_recall: ', best_recall)\n",
        "print('best_f1: ', best_f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
